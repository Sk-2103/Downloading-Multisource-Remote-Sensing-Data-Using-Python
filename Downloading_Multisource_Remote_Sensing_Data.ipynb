{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef8fd",
   "metadata": {},
   "source": [
    "# Downloading Multisource Remote Sensing Data Using Python\n",
    "\n",
    "This notebook demonstrates how to download and process various remote sensing datasets, including Sentinel-1, Sentinel-2, MODIS, DEM, and CHIRPS data. The focus is on simplifying the workflow for accessing these datasets using Python. \n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.7 or later\n",
    "- Required Libraries: `rasterio`, `geopandas`, `asf_search`, `modis_tools`, etc.\n",
    "- Internet access for downloading datasets\n",
    "\n",
    "### Structure\n",
    "1. **Sentinel-2 Data**\n",
    "2. **Digital Elevation Model (DEM)**\n",
    "3. **Precipitation Data (CHIRPS)**\n",
    "4. **MODIS Data**\n",
    "5. **Sentinel-1 Data**\n",
    "\n",
    "Each section includes example code for querying, downloading, and processing data. Comments and references are provided to guide users through the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6566d",
   "metadata": {},
   "source": [
    "## Taking a look at available Sen2 dataset's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80440f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pystac_client import Client\n",
    "\n",
    "# Load the AOI from a shapefile\n",
    "shapefile_path = \"path/shapefile/combined_extent.shp\"  # Replace with the path to your shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "geometry = gdf.geometry.iloc[0].__geo_interface__  # Assuming the AOI is the first geometry in the shapefile\n",
    "\n",
    "# Use publicly available STAC link\n",
    "client = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# ID of the collection\n",
    "collection = \"sentinel-2-l2a\"\n",
    "\n",
    "# Date range\n",
    "date_range = \"2022-09-01/2022-10-15\"\n",
    "\n",
    "# Run pystac client search without filters to check available cloud cover\n",
    "search = client.search(\n",
    "    collections=[collection], intersects=geometry, datetime=date_range\n",
    ")\n",
    "\n",
    "# Check metadata to understand cloud cover properties\n",
    "print(\"Available scenes and their metadata:\")\n",
    "for item in search.items():\n",
    "    print(f\"Scene ID: {item.id}\")\n",
    "    for key, value in item.properties.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057d2e2",
   "metadata": {},
   "source": [
    "## Downloading dataset with specified filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from pystac_client import Client\n",
    "### This is publically available dataset at AWS dont need any credentianls\n",
    "# Load the AOI from a shapefile\n",
    "shapefile_path = \"path/shapefile/combined_extent.shp\"  # Replace with the path to your shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "geometry = gdf.geometry.iloc[0].__geo_interface__  # Assuming the AOI is the first geometry in the shapefile\n",
    "\n",
    "# Use publicly available STAC link\n",
    "client = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# ID of the collection\n",
    "collection = \"sentinel-2-l2a\"\n",
    "\n",
    "# Date range\n",
    "date_range = \"2022-09-01/2022-11-15\"\n",
    "\n",
    "# Define filters (e.g., cloud cover less than 20%)\n",
    "filters = {\n",
    "    \"eo:cloud_cover\": {\"lt\": 25}  # Using an integer for the cloud cover filter\n",
    "}\n",
    "\n",
    "# Run pystac client search with the filters\n",
    "search = client.search(\n",
    "    collections=[collection],\n",
    "    intersects=geometry,\n",
    "    datetime=date_range,\n",
    "    query=filters\n",
    ")m\n",
    "\n",
    "# Get the total number of scenes found\n",
    "total_scenes = len(list(search.items()))\n",
    "print(f\"Total scenes found with the specified criteria: {total_scenes}\")\n",
    "\n",
    "# Specify the local directory where you want to save the data\n",
    "local_dir = \"path/download/Sen2\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Extract, download assets, and organize them per scene\n",
    "for item in search.items():\n",
    "    try:\n",
    "        # Create a directory for the scene using the item ID\n",
    "        scene_dir = os.path.join(local_dir, item.id)\n",
    "        os.makedirs(scene_dir, exist_ok=True)\n",
    "        \n",
    "        # Save metadata information to a text file\n",
    "        metadata_filename = os.path.join(scene_dir, f\"{item.id}_metadata.txt\")\n",
    "        with open(metadata_filename, 'w') as meta_file:\n",
    "            meta_file.write(f\"Scene ID: {item.id}\\n\")\n",
    "            meta_file.write(f\"Date and Time: {item.datetime}\\n\")\n",
    "            meta_file.write(f\"Cloud Cover: {item.properties.get('eo:cloud_cover', 'N/A')}\\n\")\n",
    "            meta_file.write(f\"Other Metadata: {item.properties}\\n\")\n",
    "\n",
    "        # Download and save assets\n",
    "        for asset_key, asset in item.assets.items():\n",
    "            # Skip unnecessary files\n",
    "            if asset_key in ['granule_metadata', 'thumbnail', 'tileinfo_metadata']:\n",
    "                continue\n",
    "            \n",
    "            # Get the URL of the asset\n",
    "            url = asset.href\n",
    "            # Create a local filename with the scene ID as prefix\n",
    "            local_filename = os.path.join(scene_dir, f\"{item.id}_{asset_key}.tif\")\n",
    "            \n",
    "            # Download the asset\n",
    "            print(f\"Downloading {url} to {local_filename}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(local_filename, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {local_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {url} with status code {response.status_code}\")\n",
    "\n",
    "        # Optionally zip the scene directory without deleting the original files\n",
    "        zip_filename = os.path.join(local_dir, f\"{item.id}.zip\")\n",
    "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "            for root, _, files in os.walk(scene_dir):\n",
    "                for file in files:\n",
    "                    zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), scene_dir))\n",
    "        \n",
    "        print(f\"Scene {item.id} zipped as {zip_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing scene {item.id}: {e}\")\n",
    "\n",
    "print(\"Download and zipping complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113e1e4",
   "metadata": {},
   "source": [
    "## Downloading Sen2 data for multiple shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ec846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from pystac_client import Client\n",
    "\n",
    "# Path to the directory containing all shapefiles\n",
    "shapefile_dir = \"path/Sen2/AOI\"  # Replace with the path to your shapefile directory\n",
    "local_dir_base = \"path/Sen2/download/Sen2\"  # Base directory to save downloaded data\n",
    "\n",
    "# Public STAC link\n",
    "client = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# ID of the collection\n",
    "collection = \"sentinel-2-l2a\"\n",
    "\n",
    "# Date range\n",
    "date_range = \"2022-09-01/2022-10-15\"\n",
    "\n",
    "# Define filters (e.g., cloud cover less than 25%)\n",
    "filters = {\n",
    "    \"eo:cloud_cover\": {\"lt\": 25}\n",
    "}\n",
    "\n",
    "# Loop through all shapefiles in the directory\n",
    "for shapefile_name in os.listdir(shapefile_dir):\n",
    "    if shapefile_name.endswith(\".shp\"):\n",
    "        shapefile_path = os.path.join(shapefile_dir, shapefile_name)\n",
    "        shapefile_base_name = os.path.splitext(shapefile_name)[0]  # Get shapefile name without extension\n",
    "        \n",
    "        # Load the AOI from the shapefile\n",
    "        gdf = gpd.read_file(shapefile_path)\n",
    "        geometry = gdf.geometry.iloc[0].__geo_interface__  # Assuming the AOI is the first geometry\n",
    "\n",
    "        # Run pystac client search with the filters\n",
    "        search = client.search(\n",
    "            collections=[collection],\n",
    "            intersects=geometry,\n",
    "            datetime=date_range,\n",
    "            query=filters\n",
    "        )\n",
    "\n",
    "        # Get the total number of scenes found\n",
    "        total_scenes = len(list(search.items()))\n",
    "        print(f\"Total scenes found for {shapefile_name} with the specified criteria: {total_scenes}\")\n",
    "\n",
    "        # Create a directory for each shapefile\n",
    "        local_dir = os.path.join(local_dir_base, shapefile_base_name)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Extract, download assets, and organize them per scene\n",
    "        for item in search.items():\n",
    "            try:\n",
    "                # Create a directory for the scene using the item ID\n",
    "                scene_dir = os.path.join(local_dir, item.id)\n",
    "                os.makedirs(scene_dir, exist_ok=True)\n",
    "\n",
    "                # Save metadata information to a text file\n",
    "                metadata_filename = os.path.join(scene_dir, f\"{item.id}_metadata.txt\")\n",
    "                with open(metadata_filename, 'w') as meta_file:\n",
    "                    meta_file.write(f\"Scene ID: {item.id}\\n\")\n",
    "                    meta_file.write(f\"Date and Time: {item.datetime}\\n\")\n",
    "                    meta_file.write(f\"Cloud Cover: {item.properties.get('eo:cloud_cover', 'N/A')}\\n\")\n",
    "                    meta_file.write(f\"Other Metadata: {item.properties}\\n\")\n",
    "\n",
    "                # Download and save assets\n",
    "                for asset_key, asset in item.assets.items():\n",
    "                    # Skip unnecessary files\n",
    "                    if asset_key in ['granule_metadata', 'thumbnail', 'tileinfo_metadata']:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get the URL of the asset\n",
    "                    url = asset.href\n",
    "                    # Create a local filename with the scene ID as prefix\n",
    "                    local_filename = os.path.join(scene_dir, f\"{item.id}_{asset_key}.tif\")\n",
    "                    \n",
    "                    # Download the asset\n",
    "                    print(f\"Downloading {url} to {local_filename}...\")\n",
    "                    response = requests.get(url, stream=True)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(local_filename, 'wb') as f:\n",
    "                            for chunk in response.iter_content(chunk_size=8192):\n",
    "                                f.write(chunk)\n",
    "                        print(f\"Downloaded {local_filename}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {url} with status code {response.status_code}\")\n",
    "\n",
    "                # Optionally zip the scene directory without deleting the original files\n",
    "                zip_filename = os.path.join(local_dir, f\"{item.id}.zip\")\n",
    "                with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "                    for root, _, files in os.walk(scene_dir):\n",
    "                        for file in files:\n",
    "                            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), scene_dir))\n",
    "\n",
    "                print(f\"Scene {item.id} zipped as {zip_filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing scene {item.id}: {e}\")\n",
    "\n",
    "print(\"Download and zipping complete for all shapefiles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc386e1",
   "metadata": {},
   "source": [
    "## Checking overlap with shapefile before download and using multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from pystac_client import Client\n",
    "from shapely.geometry import shape\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Path to the directory containing all shapefiles\n",
    "shapefile_dir = \"path/Sen2/AOI\"  # Replace with the path to your shapefile directory\n",
    "local_dir_base = \"path/Sen2/download/Sen2_test\"  # Base directory to save downloaded data\n",
    "\n",
    "# Public STAC link\n",
    "client = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# ID of the collection\n",
    "collection = \"sentinel-2-l2a\"\n",
    "\n",
    "# Date range\n",
    "date_range = \"2022-09-01/2022-10-15\"\n",
    "\n",
    "# Define filters (e.g., cloud cover less than 25%)\n",
    "filters = {\n",
    "    \"eo:cloud_cover\": {\"lt\": 45}\n",
    "}\n",
    "\n",
    "# Function to check if the scene geometry contains the shapefile geometry (100% overlap)\n",
    "def is_full_overlap(scene_geometry, aoi_geometry):\n",
    "    scene_shape = shape(scene_geometry)\n",
    "    return scene_shape.contains(aoi_geometry)  # Check if the scene contains the AOI\n",
    "\n",
    "# Function to perform pre-check: ensure all shapefiles have 100% overlap\n",
    "def precheck_shapefile_overlap(shapefile_name):\n",
    "    shapefile_path = os.path.join(shapefile_dir, shapefile_name)\n",
    "    shapefile_base_name = os.path.splitext(shapefile_name)[0]  # Get shapefile name without extension\n",
    "\n",
    "    # Load the AOI from the shapefile\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    aoi_geometry = gdf.geometry.iloc[0]  # Assuming the AOI is the first geometry\n",
    "\n",
    "    # Run pystac client search with the filters\n",
    "    search = client.search(\n",
    "        collections=[collection],\n",
    "        intersects=aoi_geometry.__geo_interface__,  # Query based on the AOI geometry\n",
    "        datetime=date_range,\n",
    "        query=filters\n",
    "    )\n",
    "\n",
    "    # Inform user how many scenes found before filtering for 100% overlap\n",
    "    all_scenes = list(search.items())\n",
    "    print(f\"Total scenes found for {shapefile_name} before 100% overlap check: {len(all_scenes)}\")\n",
    "\n",
    "    # Filter for scenes that fully contain the AOI\n",
    "    valid_scenes = []\n",
    "    for item in all_scenes:\n",
    "        scene_geometry = item.geometry\n",
    "        if is_full_overlap(scene_geometry, aoi_geometry):\n",
    "            valid_scenes.append(item)\n",
    "\n",
    "    # Return True if at least one valid scene is found, else False\n",
    "    if len(valid_scenes) == 0:\n",
    "        print(f\"No fully overlapping scenes found for {shapefile_name}.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Total scenes with 100% overlap for {shapefile_name}: {len(valid_scenes)}\")\n",
    "        return True\n",
    "\n",
    "# Function to process downloading and zipping for a single scene\n",
    "def process_scene(item, scene_dir, local_dir):\n",
    "    try:\n",
    "        # Create a directory for the scene using the item ID\n",
    "        os.makedirs(scene_dir, exist_ok=True)\n",
    "\n",
    "        # Save metadata information to a text file\n",
    "        metadata_filename = os.path.join(scene_dir, f\"{item.id}_metadata.txt\")\n",
    "        with open(metadata_filename, 'w') as meta_file:\n",
    "            meta_file.write(f\"Scene ID: {item.id}\\n\")\n",
    "            meta_file.write(f\"Date and Time: {item.datetime}\\n\")\n",
    "            meta_file.write(f\"Cloud Cover: {item.properties.get('eo:cloud_cover', 'N/A')}\\n\")\n",
    "            meta_file.write(f\"Other Metadata: {item.properties}\\n\")\n",
    "\n",
    "        # Download and save assets\n",
    "        for asset_key, asset in item.assets.items():\n",
    "            # Skip unnecessary files\n",
    "            if asset_key in ['granule_metadata', 'thumbnail', 'tileinfo_metadata']:\n",
    "                continue\n",
    "\n",
    "            # Get the URL of the asset\n",
    "            url = asset.href\n",
    "            # Create a local filename with the scene ID as prefix\n",
    "            local_filename = os.path.join(scene_dir, f\"{item.id}_{asset_key}.tif\")\n",
    "\n",
    "            # Download the asset\n",
    "            print(f\"Downloading {url} to {local_filename}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(local_filename, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Downloaded {local_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {url} with status code {response.status_code}\")\n",
    "\n",
    "        # Optionally zip the scene directory without deleting the original files\n",
    "        zip_filename = os.path.join(local_dir, f\"{item.id}.zip\")\n",
    "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "            for root, _, files in os.walk(scene_dir):\n",
    "                for file in files:\n",
    "                    zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), scene_dir))\n",
    "\n",
    "        print(f\"Scene {item.id} zipped as {zip_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing scene {item.id}: {e}\")\n",
    "\n",
    "# Function to handle all scenes for a single shapefile\n",
    "def process_shapefile(shapefile_name):\n",
    "    shapefile_path = os.path.join(shapefile_dir, shapefile_name)\n",
    "    shapefile_base_name = os.path.splitext(shapefile_name)[0]  # Get shapefile name without extension\n",
    "\n",
    "    # Load the AOI from the shapefile\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    aoi_geometry = gdf.geometry.iloc[0]  # Assuming the AOI is the first geometry\n",
    "\n",
    "    # Run pystac client search with the filters\n",
    "    search = client.search(\n",
    "        collections=[collection],\n",
    "        intersects=aoi_geometry.__geo_interface__,  # Query based on the AOI geometry\n",
    "        datetime=date_range,\n",
    "        query=filters\n",
    "    )\n",
    "\n",
    "    # Filter for scenes that fully contain the AOI\n",
    "    valid_scenes = []\n",
    "    for item in search.items():\n",
    "        scene_geometry = item.geometry\n",
    "        if is_full_overlap(scene_geometry, aoi_geometry):\n",
    "            valid_scenes.append(item)\n",
    "\n",
    "    total_scenes = len(valid_scenes)\n",
    "\n",
    "    print(f\"Total scenes with 100% overlap for {shapefile_name}: {total_scenes}\")\n",
    "\n",
    "    # Create a directory for each shapefile\n",
    "    local_dir = os.path.join(local_dir_base, shapefile_base_name)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    # Process each valid scene in parallel\n",
    "    Parallel(n_jobs=3)(delayed(process_scene)(item, os.path.join(local_dir, item.id), local_dir) for item in valid_scenes)\n",
    "\n",
    "\n",
    "# Main logic: Pre-check if all shapefiles have at least one 100% overlap scene before proceeding with downloads\n",
    "if __name__ == \"__main__\":\n",
    "    shapefiles = [f for f in os.listdir(shapefile_dir) if f.endswith(\".shp\")]\n",
    "    \n",
    "    # Step 1: Pre-check for 100% overlap for all shapefiles\n",
    "    all_overlap = True\n",
    "    for shapefile_name in shapefiles:\n",
    "        if not precheck_shapefile_overlap(shapefile_name):\n",
    "            all_overlap = False\n",
    "    \n",
    "    if not all_overlap:\n",
    "        print(\"Some shapefiles do not have 100% overlap scenes. Halting the process.\")\n",
    "    else:\n",
    "        # Step 2: Proceed with downloading if all shapefiles have 100% overlap scenes\n",
    "        print(\"All shapefiles have 100% overlap. Proceeding with downloads.\")\n",
    "        Parallel(n_jobs=3)(delayed(process_shapefile)(shapefile_name) for shapefile_name in shapefiles)\n",
    "\n",
    "    print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177402f",
   "metadata": {},
   "source": [
    "## Downloading Corresponding DEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No login credentials required\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import floor\n",
    "import geopandas as gpd\n",
    "from subprocess import call\n",
    "\n",
    "# Define the AOI and destination CRS\n",
    "shapefile_path = \"ex/shapefile/combined_extent.shp\"  # Replace with your shapefile path\n",
    "destination_crs = \"EPSG:4326\"  # Can be None if CRS conversion is not needed\n",
    "\n",
    "# Define paths\n",
    "base_folder = \"ex/download/DEM\"\n",
    "download_path = f'{base_folder}/dems/copernicus'\n",
    "crs_conversion_path = f'{base_folder}/dems/crs_conversion'\n",
    "\n",
    "# Create the DEM folder if necessary\n",
    "if not os.path.exists(download_path):\n",
    "    print(f\"Creating Folder(s): {download_path}\")\n",
    "    os.makedirs(download_path)\n",
    "\n",
    "# Load the AOI from a shapefile\n",
    "aoi = gpd.read_file(shapefile_path)\n",
    "aoi = aoi.to_crs('EPSG:4326')\n",
    "\n",
    "# Function to construct the full path to the DEM on AWS S3\n",
    "def s3Path(lat, lon):\n",
    "    \"\"\" \n",
    "    Construct full path to data, using longitude and latitude.\n",
    "    \"\"\"\n",
    "    lonSign = {1: 'E', -1: 'W', 0: 'E'}\n",
    "    latSign = {1: 'N', -1: 'S', 0: 'N'}\n",
    "\n",
    "    lonStr = f'{lonSign[np.sign(lon)]}{np.abs(lon):03}' # The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0\n",
    "    latStr =  f'{latSign[np.sign(lat)]}{np.abs(lat):02}'\n",
    "\n",
    "    myPath = f's3://copernicus-dem-30m/Copernicus_DSM_COG_10_{latStr}_00_{lonStr}_00_DEM/Copernicus_DSM_COG_10_{latStr}_00_{lonStr}_00_DEM.tif'\n",
    "    \n",
    "    return myPath\n",
    "\n",
    "# Download DEM data for each point in the AOI\n",
    "\n",
    "# For each record in the AOI gdf\n",
    "for row in aoi.itertuples():\n",
    "    # Get the geometry bounds for that record.\n",
    "    geom = row.geometry\n",
    "    xmin, ymin, xmax, ymax = geom.bounds\n",
    "\n",
    "    # Add a buffer to the bounds to ensure we cover the area\n",
    "    buffer_degree = 0.05\n",
    "    xmin -= buffer_degree\n",
    "    ymin -= buffer_degree\n",
    "    xmax += buffer_degree\n",
    "    ymax += buffer_degree\n",
    "\n",
    "    # Get a list of every whole-degree longitude and latitude in the buffered bounds.\n",
    "    lons = list(set([int(floor(xmin)), int(floor(xmax))]))\n",
    "    lats = list(set([int(floor(ymin)), int(floor(ymax))]))\n",
    "\n",
    "    # For each lon/lat combination, call the AWS CLI to download the corresponding DEM data.\n",
    "    for lon in lons:\n",
    "        for lat in lats:\n",
    "            myPath = s3Path(lat, lon)\n",
    "            myFile = myPath.split('/')[-1]\n",
    "\n",
    "            if not os.path.exists(f'{download_path}/{myFile}'):\n",
    "                print(f\"Downloading: {myPath}\")\n",
    "                command = f'aws s3 cp {myPath} {download_path} --no-sign-request'\n",
    "                print(command)\n",
    "                call(command, shell=True)\n",
    "            else:\n",
    "                print(f'Already exists: {myFile}')\n",
    "\n",
    "# Optionally convert CRS if a destination CRS is specified\n",
    "if destination_crs is not None:\n",
    "    # convert_tif_folder_to_crs(download_path, crs_conversion_path, destination_crs)  # Commented out for now\n",
    "    print(f\"Skipping CRS conversion; function not implemented.\")\n",
    "\n",
    "print('END of Downloads')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86133b",
   "metadata": {},
   "source": [
    "## Downloading DEM for mutiple shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0917e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import floor\n",
    "import geopandas as gpd\n",
    "from subprocess import call\n",
    "\n",
    "# Define the base paths\n",
    "shapefile_dir = \"ex/Sen2/shapefile/\"  # Folder containing all shapefiles\n",
    "destination_crs = \"EPSG:4326\"  # Can be None if CRS conversion is not needed\n",
    "\n",
    "# Define base paths for downloads\n",
    "base_folder = \"ex/Sen2/download/DEM\"\n",
    "download_base_path = f'{base_folder}/dems'\n",
    "\n",
    "# Function to construct the full path to the DEM on AWS S3\n",
    "def s3Path(lat, lon):\n",
    "    \"\"\" \n",
    "    Construct full path to data, using longitude and latitude.\n",
    "    \"\"\"\n",
    "    lonSign = {1: 'E', -1: 'W', 0: 'E'}\n",
    "    latSign = {1: 'N', -1: 'S', 0: 'N'}\n",
    "\n",
    "    lonStr = f'{lonSign[np.sign(lon)]}{np.abs(lon):03}'  # The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0\n",
    "    latStr =  f'{latSign[np.sign(lat)]}{np.abs(lat):02}'\n",
    "\n",
    "    myPath = f's3://copernicus-dem-30m/Copernicus_DSM_COG_10_{latStr}_00_{lonStr}_00_DEM/Copernicus_DSM_COG_10_{latStr}_00_{lonStr}_00_DEM.tif'\n",
    "    \n",
    "    return myPath\n",
    "\n",
    "# Function to process each shapefile\n",
    "def process_shapefile(shapefile_path):\n",
    "    shapefile_name = os.path.splitext(os.path.basename(shapefile_path))[0]  # Extract shapefile name without extension\n",
    "    download_path = f'{download_base_path}/{shapefile_name}'\n",
    "    \n",
    "    # Create the download folder if necessary\n",
    "    if not os.path.exists(download_path):\n",
    "        print(f\"Creating Folder(s): {download_path}\")\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # Load the AOI from a shapefile\n",
    "    aoi = gpd.read_file(shapefile_path)\n",
    "    aoi = aoi.to_crs(destination_crs)\n",
    "\n",
    "    # For each record in the AOI gdf\n",
    "    for row in aoi.itertuples():\n",
    "        # Get the geometry bounds for that record.\n",
    "        geom = row.geometry\n",
    "        xmin, ymin, xmax, ymax = geom.bounds\n",
    "\n",
    "        # Add a buffer to the bounds to ensure we cover the area\n",
    "        buffer_degree = 0.05\n",
    "        xmin -= buffer_degree\n",
    "        ymin -= buffer_degree\n",
    "        xmax += buffer_degree\n",
    "        ymax += buffer_degree\n",
    "\n",
    "        # Get a list of every whole-degree longitude and latitude in the buffered bounds.\n",
    "        lons = list(set([int(floor(xmin)), int(floor(xmax))]))\n",
    "        lats = list(set([int(floor(ymin)), int(floor(ymax))]))\n",
    "\n",
    "        # For each lon/lat combination, call the AWS CLI to download the corresponding DEM data.\n",
    "        for lon in lons:\n",
    "            for lat in lats:\n",
    "                myPath = s3Path(lat, lon)\n",
    "                myFile = myPath.split('/')[-1]\n",
    "\n",
    "                if not os.path.exists(f'{download_path}/{myFile}'):\n",
    "                    print(f\"Downloading: {myPath}\")\n",
    "                    command = f'aws s3 cp {myPath} {download_path} --no-sign-request'\n",
    "                    print(command)\n",
    "                    call(command, shell=True)\n",
    "                else:\n",
    "                    print(f'Already exists: {myFile}')\n",
    "                    \n",
    "    print(f\"Finished downloading DEMs for {shapefile_name}\")\n",
    "\n",
    "# Get all shapefiles in the directory\n",
    "shapefiles = [os.path.join(shapefile_dir, f) for f in os.listdir(shapefile_dir) if f.endswith('.shp')]\n",
    "\n",
    "# Process each shapefile\n",
    "for shapefile_path in shapefiles:\n",
    "    process_shapefile(shapefile_path)\n",
    "\n",
    "print('END of Downloads')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a6209",
   "metadata": {},
   "source": [
    "## Clipping DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional, this code clip DEMs using shapefile\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from rasterio.mask import mask\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to check and reproject shapefile if CRS doesn't match with raster\n",
    "def check_and_reproject_raster(tif_file, shapefile):\n",
    "    # Open the shapefile\n",
    "    shp = gpd.read_file(shapefile)\n",
    "    \n",
    "    # Open the raster file\n",
    "    with rasterio.open(tif_file) as src:\n",
    "        raster_crs = src.crs\n",
    "\n",
    "    # Get the CRS of the shapefile\n",
    "    shapefile_crs = shp.crs\n",
    "\n",
    "    # Check if CRS match\n",
    "    if raster_crs != shapefile_crs:\n",
    "        print(f\"CRS Mismatch: Raster CRS is {raster_crs}, Shapefile CRS is {shapefile_crs}\")\n",
    "        # Reproject shapefile to match raster CRS\n",
    "        shp = shp.to_crs(raster_crs)\n",
    "        print(\"Shapefile reprojected to match raster CRS.\")\n",
    "        return shp\n",
    "    else:\n",
    "        print(\"CRS match.\")\n",
    "        return shp\n",
    "\n",
    "# Function to clip shapefile to raster bounds\n",
    "def clip_shapefile_to_raster(tif_file, shapefile):\n",
    "    # Open the raster to get its bounds\n",
    "    with rasterio.open(tif_file) as src:\n",
    "        raster_bounds = src.bounds\n",
    "        # Use shapely.geometry.box() to create a bounding box\n",
    "        raster_bbox = gpd.GeoDataFrame({\"geometry\": [box(*raster_bounds)]}, crs=src.crs)\n",
    "\n",
    "    # Open the shapefile and clip it\n",
    "    shp = gpd.read_file(shapefile)\n",
    "\n",
    "    # If the shapefile CRS doesn't match, reproject it\n",
    "    if shp.crs != raster_bbox.crs:\n",
    "        shp = shp.to_crs(raster_bbox.crs)\n",
    "\n",
    "    # Clip the shapefile using the raster bounds\n",
    "    clipped_shapefile = gpd.overlay(shp, raster_bbox, how='intersection')\n",
    "\n",
    "    return clipped_shapefile\n",
    "\n",
    "# Function to recursively find all .tif files ending with 'DEM.tif'\n",
    "def find_tif_files(root_dir):\n",
    "    tif_files = []\n",
    "    valid_suffixes = ['DEM.tif']\n",
    "    \n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if any(file.endswith(suffix) for suffix in valid_suffixes):\n",
    "                tif_files.append(os.path.join(dirpath, file))\n",
    "    return tif_files\n",
    "\n",
    "# Function to clip a raster with a shapefile and save it with the shapefile name\n",
    "def clip_raster_with_shapefile(tif_file, shapefile, output_dir):\n",
    "    # Check and reproject shapefile to match raster CRS\n",
    "    shp = check_and_reproject_raster(tif_file, shapefile)\n",
    "    \n",
    "    # Clip the shapefile to the raster bounds\n",
    "    shp = clip_shapefile_to_raster(tif_file, shapefile)\n",
    "\n",
    "    with rasterio.open(tif_file) as src:\n",
    "        shapes = [geom for geom in shp.geometry]\n",
    "        \n",
    "        if len(shapes) == 0:\n",
    "            print(f\"No overlap between {tif_file} and the shapefile.\")\n",
    "            return\n",
    "        \n",
    "        out_image, out_transform = mask(src, shapes, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform\n",
    "        })\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extract the shapefile name (without extension) to use it as the output filename\n",
    "        shapefile_name = os.path.splitext(os.path.basename(shapefile))[0]\n",
    "        output_file = os.path.join(output_dir, f\"{shapefile_name}_DEM.tif\")\n",
    "        \n",
    "        # Save the clipped raster\n",
    "        with rasterio.open(output_file, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "# Function to match root folder names with shapefiles and clip rasters in subfolders\n",
    "def clip_all_tifs_with_matching_shapefile(tif_root_dir, shapefile_root_dir):\n",
    "    # Iterate over the subfolders in the tif_root_dir\n",
    "    for root_folder in os.listdir(tif_root_dir):\n",
    "        root_folder_path = os.path.join(tif_root_dir, root_folder)\n",
    "        \n",
    "        # Check if the root folder is indeed a directory\n",
    "        if os.path.isdir(root_folder_path):\n",
    "            # Try to find a shapefile in the shapefile root directory with the same name as the root folder\n",
    "            shapefile_path = os.path.join(shapefile_root_dir, f\"{root_folder}.shp\")\n",
    "            \n",
    "            if os.path.exists(shapefile_path):\n",
    "                print(f\"Using shapefile: {shapefile_path} for folder: {root_folder}\")\n",
    "                \n",
    "                # Find all .tif files within this root folder (including subfolders)\n",
    "                tif_files = find_tif_files(root_folder_path)\n",
    "                \n",
    "                # Process each .tif file\n",
    "                for tif_file in tif_files:\n",
    "                    # Recreate the folder structure in the output directory\n",
    "                    relative_path = os.path.relpath(tif_file, tif_root_dir)\n",
    "                    output_dir = os.path.join(tif_root_dir, \"clipped\", os.path.dirname(relative_path))\n",
    "                    \n",
    "                    # Clip each raster using the matched shapefile\n",
    "                    clip_raster_with_shapefile(tif_file, shapefile_path, output_dir)\n",
    "                    print(f\"Clipped: {tif_file} and saved to {output_dir}\")\n",
    "            else:\n",
    "                print(f\"No matching shapefile found for folder: {root_folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the root directory containing the .tif files and the shapefile root directory\n",
    "    tif_root_directory = r\"path\\download\\DEM\\dems\"  # Your specified root directory for .tif files\n",
    "    shapefile_root_directory = r\"path\\AOI\"  # Your specified root directory for shapefiles\n",
    "\n",
    "    # Call the function to clip all .tif files with matching shapefiles\n",
    "    clip_all_tifs_with_matching_shapefile(tif_root_directory, shapefile_root_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12e97f",
   "metadata": {},
   "source": [
    "## Dwonloading Precipitation (CHIRPS) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbddcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No login credential required, download CIRPS precipitation data\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "base_folder = \"path/download/CHIRPS\"\n",
    "download_path = f'{base_folder}/daily'\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Load the AOI from a shapefile and reproject to WGS 84 (EPSG:4326)\n",
    "shapefile_path = \"path/shapefile/combined_extent.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf.to_crs('EPSG:4326')  # Reproject to the CRS used by CHIRPS\n",
    "\n",
    "geometry = gdf.geometry\n",
    "\n",
    "# Function to download CHIRPS data via HTTP\n",
    "def download_chirps_http(date):\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    day = date[8:10]\n",
    "    filename = f'chirps-v2.0.{year}.{month}.{day}.tif.gz'\n",
    "    url = f'http://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/tifs/p05/{year}/{filename}'\n",
    "    local_gz_path = os.path.join(download_path, filename)\n",
    "\n",
    "    if not os.path.exists(local_gz_path):\n",
    "        print(f'Downloading: {url}')\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_gz_path, 'wb') as f_out:\n",
    "                f_out.write(response.content)\n",
    "            print(f'Downloaded {local_gz_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {url} with status code {response.status_code}')\n",
    "    else:\n",
    "        print(f'Already exists: {local_gz_path}')\n",
    "\n",
    "# Define date range (example: 2022-01-01 to 2022-01-31)\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2022-01-10'\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Loop through the date range and download data for each date\n",
    "for date in date_range:\n",
    "    download_chirps_http(date.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Unzip and clip downloaded files\n",
    "for gz_file in os.listdir(download_path):\n",
    "    if gz_file.endswith('.gz'):\n",
    "        gz_path = os.path.join(download_path, gz_file)\n",
    "        tif_path = gz_path[:-3]  # Remove .gz extension\n",
    "\n",
    "        # Unzipping\n",
    "        if not os.path.exists(tif_path):\n",
    "            print(f'Unzipping {gz_path}')\n",
    "            with gzip.open(gz_path, 'rb') as f_in:\n",
    "                with open(tif_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        # Clip to AOI using the exact shape\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            out_image, out_transform = mask(src, geometry, crop=True, filled=True)\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\": out_image.shape[2],\n",
    "                \"transform\": out_transform\n",
    "            })\n",
    "        \n",
    "            #clipped_path = os.path.join(download_path, f'clipped_{os.path.basename(tif_path)}')\n",
    "            #with rasterio.open(clipped_path, \"w\", **out_meta) as dest:\n",
    "                #dest.write(out_image)\n",
    "            #print(f'Clipped data saved to {clipped_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961869d",
   "metadata": {},
   "source": [
    "## Clipping chirps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbdc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##optical, this code clip CHIRPS dataset using shapefile\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "\n",
    "def clip_raster_with_shapefiles(tif_folder, shapefile_folder, output_folder):\n",
    "    # List all .tif files in the raster folder\n",
    "    tif_files = [f for f in os.listdir(tif_folder) if f.endswith('.tif')]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # List all shapefiles in the folder\n",
    "    shapefiles = [f for f in os.listdir(shapefile_folder) if f.endswith('.shp')]\n",
    "    \n",
    "    # Loop through each .tif file\n",
    "    for tif_file in tif_files:\n",
    "        tif_path = os.path.join(tif_folder, tif_file)\n",
    "        \n",
    "        # Extract the full date from the .tif file name (assumes 'chirps-v2.0.2022.01.01.tif' format)\n",
    "        tif_filename = os.path.basename(tif_path)\n",
    "        # Adjusting the index to get the full date (3rd part from the split)\n",
    "        date_str = '.'.join(tif_filename.split('.')[2:5])  # Extracts '2022.01.01'\n",
    "        \n",
    "        # Open the raster file\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            # Loop through each shapefile\n",
    "            for shapefile in shapefiles:\n",
    "                shapefile_path = os.path.join(shapefile_folder, shapefile)\n",
    "                # Read the shapefile using Geopandas\n",
    "                shapes = gpd.read_file(shapefile_path)\n",
    "                \n",
    "                # Reproject shapefile to the same CRS as the raster\n",
    "                shapes = shapes.to_crs(src.crs)\n",
    "\n",
    "                # Convert the shapes to GeoJSON-like format for rasterio\n",
    "                geoms = shapes.geometry.values\n",
    "                geoms = [geom.__geo_interface__ for geom in geoms]\n",
    "\n",
    "                # Clip the raster using the geometry\n",
    "                out_image, out_transform = mask(src, geoms, crop=True)\n",
    "\n",
    "                # Update metadata\n",
    "                out_meta = src.meta.copy()\n",
    "                out_meta.update({\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": out_image.shape[1],\n",
    "                    \"width\": out_image.shape[2],\n",
    "                    \"transform\": out_transform\n",
    "                })\n",
    "\n",
    "                # Create the output filename based on the shapefile name and date\n",
    "                shapefile_name = os.path.splitext(shapefile)[0]\n",
    "                output_tif_name = f\"{shapefile_name}_{date_str}.tif\"\n",
    "                output_tif_path = os.path.join(output_folder, output_tif_name)\n",
    "\n",
    "                # Save the clipped raster\n",
    "                with rasterio.open(output_tif_path, 'w', **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "\n",
    "                print(f\"Saved clipped raster: {output_tif_path}\")\n",
    "\n",
    "# Example usage:\n",
    "tif_folder = 'path/CHIRPS/daily'\n",
    "shapefile_folder = r'path/AOI'\n",
    "output_folder = 'path/download/processed/precipitation'\n",
    "\n",
    "clip_raster_with_shapefiles(tif_folder, shapefile_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98829a09",
   "metadata": {},
   "source": [
    "## Downloading MODIS data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code make sure of download band1 and band2 (MOD09GQ) at 250m and band7 at 500m (MOD09GA). Run as per requirement.\n",
    "\n",
    "from modis_tools.auth import ModisSession\n",
    "from modis_tools.resources import CollectionApi, GranuleApi\n",
    "from modis_tools.granule_handler import GranuleHandler\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import geopandas as gpd\n",
    "\n",
    "# Set your Earthdata username and password\n",
    "username = \"your_username\"  # Update this line\n",
    "password = \"your_password\"  # Update this line\n",
    "\n",
    "# Initialize the MODIS session\n",
    "session = ModisSession(username=username, password=password)\n",
    "\n",
    "# Define the local directory where data will be downloaded\n",
    "download_directory = \"path/to/data/download/MODIS/\"\n",
    "os.makedirs(download_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Load the shapefile for the AOI\n",
    "shapefile_path = \"path/to/data/shapefile/combined_extent.shp\"  # Replace with your shapefile path\n",
    "aoi = gpd.read_file(shapefile_path)\n",
    "aoi = aoi.to_crs('EPSG:4326')  # Reproject to WGS84 if needed\n",
    "bbox = tuple(aoi.total_bounds)  # Convert numpy array to tuple\n",
    "\n",
    "# Initialize CollectionApi for 250m and 500m products\n",
    "collection_client = CollectionApi(session=session)\n",
    "\n",
    "# Query for MOD09GQ (250m resolution, bands 1 and 2)\n",
    "collections_250m = collection_client.query(short_name=\"MOD09GQ\", version=\"061\")\n",
    "\n",
    "# Query for MOD09GA (500m resolution, bands 3-7)\n",
    "collections_500m = collection_client.query(short_name=\"MOD09GA\", version=\"061\")\n",
    "\n",
    "# Check if collections are found\n",
    "if not collections_250m or not collections_500m:\n",
    "    print(\"No collections found for MOD09GQ or MOD09GA. Please check the product name and version.\")\n",
    "    exit()\n",
    "\n",
    "# Function to download granules\n",
    "def download_granules(collection, start_date, end_date, bounding_box, session, download_path):\n",
    "    granule_client = GranuleApi.from_collection(collection, session=session)\n",
    "    granules = granule_client.query(start_date=start_date, end_date=end_date, bounding_box=bounding_box)\n",
    "    GranuleHandler.download_from_granules(granules, session, path=download_path)\n",
    "    return granules\n",
    "\n",
    "# Download 250m data (bands 1 and 2)\n",
    "granules_250m = download_granules(collections_250m[0], \"2022-01-01\", \"2022-01-03\", bbox, session, download_directory)\n",
    "\n",
    "# Download 500m data (bands 3-7)\n",
    "granules_500m = download_granules(collections_500m[0], \"2022-01-01\", \"2022-01-03\", bbox, session, download_directory)\n",
    "\n",
    "print(f\"Granules downloaded to {download_directory}\")\n",
    "\n",
    "# Function to extract specific bands from the .hdf files and save them in the scene directory\n",
    "def extract_bands_from_hdf(hdf_path, bands_to_extract, download_dir):\n",
    "    with rasterio.open(hdf_path) as hdf:\n",
    "        # Get all available subdatasets\n",
    "        subdatasets = hdf.subdatasets\n",
    "\n",
    "        for subdataset_name in subdatasets:\n",
    "            # Extract the name of the subdataset (e.g., 'MODIS_Grid_500m_2D:sur_refl_b01')\n",
    "            subdataset_band_name = subdataset_name.split(\":\")[-1]\n",
    "\n",
    "            if subdataset_band_name in bands_to_extract:\n",
    "                with rasterio.open(subdataset_name) as subdataset:\n",
    "                    # Define scene output directory\n",
    "                    scene_name = os.path.basename(hdf_path).replace('.hdf', '')\n",
    "                    scene_output_dir = os.path.join(download_dir, scene_name)\n",
    "                    os.makedirs(scene_output_dir, exist_ok=True)  # Create directory for each scene\n",
    "\n",
    "                    # Define output file path for the specific band\n",
    "                    output_filename = f\"{scene_name}_{subdataset_band_name}.tif\"\n",
    "                    full_output_path = os.path.join(scene_output_dir, output_filename)\n",
    "\n",
    "                    # Read the band data\n",
    "                    data = subdataset.read(\n",
    "                        out_shape=(\n",
    "                            subdataset.count,\n",
    "                            int(subdataset.height),\n",
    "                            int(subdataset.width)\n",
    "                        ),\n",
    "                        resampling=Resampling.nearest\n",
    "                    )\n",
    "                    \n",
    "                    # Write the data to a new GeoTIFF\n",
    "                    with rasterio.open(\n",
    "                        full_output_path,\n",
    "                        'w',\n",
    "                        driver='GTiff',\n",
    "                        height=subdataset.height,\n",
    "                        width=subdataset.width,\n",
    "                        count=subdataset.count,\n",
    "                        dtype=data.dtype,\n",
    "                        crs=subdataset.crs,\n",
    "                        transform=subdataset.transform,\n",
    "                    ) as dst:\n",
    "                        dst.write(data)\n",
    "                    \n",
    "                    print(f\"Extracted {subdataset_band_name} to {full_output_path}\")\n",
    "\n",
    "# List of subdataset names corresponding to bands 1 and 2 in MOD09GQ\n",
    "bands_250m = [\"sur_refl_b01\", \"sur_refl_b02\"]\n",
    "\n",
    "# List of subdataset names corresponding to bands 1, 2, and 7 in MOD09GA (500m resampling)\n",
    "bands_500m = [\"sur_refl_b01\", \"sur_refl_b02\", \"sur_refl_b07\"]\n",
    "\n",
    "# Extract bands from each downloaded 250m .hdf file and save to scene-specific folders\n",
    "for granule in granules_250m:\n",
    "    hdf_path = os.path.join(download_directory, os.path.basename(granule))\n",
    "    extract_bands_from_hdf(hdf_path, bands_250m, download_directory)\n",
    "\n",
    "# Extract bands from each downloaded 500m .hdf file and save to scene-specific folders\n",
    "for granule in granules_500m:\n",
    "    hdf_path = os.path.join(download_directory, os.path.basename(granule))\n",
    "    extract_bands_from_hdf(hdf_path, bands_500m, download_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e2094",
   "metadata": {},
   "source": [
    "## Extracting relevant bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code make sure to extract band1 and band2 at 250m and band 7 at 5000m.\n",
    "\n",
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "# Define directories\n",
    "hdf_directory = \"path/to/data/download/MODIS/\"  # Directory with .hdf files\n",
    "output_base_directory = \"path/to/data/download/MODIS/extracted_bands\"  # Base directory to save extracted bands\n",
    "os.makedirs(output_base_directory, exist_ok=True)\n",
    "\n",
    "# Define band indices for MOD09GA and MOD09GQ products\n",
    "bands_to_extract_mod09ga = {\n",
    "    \"sur_refl_b07\": 18  # Only extract band 7 for MOD09GA\n",
    "}\n",
    "\n",
    "bands_to_extract_mod09gq = {\n",
    "    \"sur_refl_b01\": 0,  # Extract bands 1 and 2 for MOD09GQ\n",
    "    \"sur_refl_b02\": 1\n",
    "}\n",
    "\n",
    "# Function to extract bands from HDF file using GDAL\n",
    "def extract_bands_from_hdf(hdf_file, bands_to_extract, scene_id):\n",
    "    try:\n",
    "        # Open the HDF file using GDAL\n",
    "        hdf_dataset = gdal.Open(hdf_file)\n",
    "\n",
    "        if hdf_dataset is None:\n",
    "            print(f\"Failed to open {hdf_file}\")\n",
    "            return\n",
    "\n",
    "        # Get subdatasets (GDAL lists all the datasets within the HDF file)\n",
    "        subdatasets = hdf_dataset.GetSubDatasets()\n",
    "\n",
    "        # Log subdatasets for debugging\n",
    "        print(f\"Subdatasets found in {hdf_file}:\")\n",
    "        for i, subdataset in enumerate(subdatasets):\n",
    "            print(f\"{i}: {subdataset[0]}\")\n",
    "\n",
    "        # Create scene output directory based on the last 13 digits of the filename\n",
    "        scene_output_directory = os.path.join(output_base_directory, scene_id)\n",
    "        os.makedirs(scene_output_directory, exist_ok=True)\n",
    "\n",
    "        for band_name, index in bands_to_extract.items():\n",
    "            # Get the subdataset path for the required band\n",
    "            if index < len(subdatasets):\n",
    "                subdataset_path = subdatasets[index][0]\n",
    "\n",
    "                # Open the subdataset\n",
    "                band_dataset = gdal.Open(subdataset_path)\n",
    "\n",
    "                if band_dataset is None:\n",
    "                    print(f\"Failed to open subdataset {band_name} in {hdf_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Construct the output filename\n",
    "                output_filename = f\"{scene_id}_{band_name}.tif\"\n",
    "                output_path = os.path.join(scene_output_directory, output_filename)\n",
    "\n",
    "                # Save the band as a GeoTIFF\n",
    "                gdal.Translate(output_path, band_dataset, format='GTiff')\n",
    "                print(f\"Saved {band_name} to {output_path}\")\n",
    "            else:\n",
    "                print(f\"Index {index} out of range for subdatasets in {hdf_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {hdf_file}: {e}\")\n",
    "\n",
    "# Loop over all HDF files in the directory\n",
    "for hdf_file in os.listdir(hdf_directory):\n",
    "    if hdf_file.endswith(\".hdf\"):\n",
    "        hdf_file_path = os.path.join(hdf_directory, hdf_file)\n",
    "\n",
    "        # Extract the last 13 digits for the scene folder name\n",
    "        scene_id = hdf_file[-17:-4]  # Last 13 digits before \".hdf\"\n",
    "\n",
    "        print(f\"Processing file: {hdf_file_path}\")\n",
    "\n",
    "        # Check if the file starts with MOD09GA or MOD09GQ\n",
    "        if hdf_file.startswith(\"MOD09GA\"):\n",
    "            # Extract only sur_refl_b07 for MOD09GA\n",
    "            extract_bands_from_hdf(hdf_file_path, bands_to_extract_mod09ga, scene_id)\n",
    "        elif hdf_file.startswith(\"MOD09GQ\"):\n",
    "            # Extract sur_refl_b01 and sur_refl_b02 for MOD09GQ\n",
    "            extract_bands_from_hdf(hdf_file_path, bands_to_extract_mod09gq, scene_id)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7d7ee",
   "metadata": {},
   "source": [
    "## Downloading Sentinel 1 GRD data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Single shapefile\n",
    "\n",
    "from os import listdir\n",
    "import getpass\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import asf_search as asf\n",
    "from datetime import datetime\n",
    "\n",
    "# User authentication\n",
    "username = input('Username:')\n",
    "password = getpass.getpass('Password:')\n",
    "\n",
    "# Start ASF session with credentials\n",
    "session = asf.ASFSession().auth_with_creds(username=username, password=password)\n",
    "\n",
    "# Load the shapefile as AOI\n",
    "shapefile_path = 'path/to/shapefiles/j3zo_extent.shp'  # Update this with the path to your shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "aoi_geom = gdf.geometry.unary_union.wkt  # Combine all geometries into one and get WKT\n",
    "\n",
    "# Define date range\n",
    "start_date = '2022-10-01'  # Update with your start date\n",
    "end_date = '2022-11-28'    # Update with your end date\n",
    "\n",
    "# Search for Sentinel-1 GRD HD data with the specified date range\n",
    "results = asf.geo_search(\n",
    "    intersectsWith=aoi_geom,\n",
    "    platform=asf.PLATFORM.SENTINEL1,\n",
    "    processingLevel=asf.PRODUCT_TYPE.GRD_HD,\n",
    "    start=datetime.strptime(start_date, '%Y-%m-%d'),\n",
    "    end=datetime.strptime(end_date, '%Y-%m-%d'),\n",
    "    maxResults=2\n",
    ")\n",
    "\n",
    "# Download with progress bar using tqdm and parallel processes\n",
    "for result in tqdm(results, desc=\"Downloading GRD HD Images\", unit=\"file\"):\n",
    "    result.download(\n",
    "        path='path/to/output/GRD',\n",
    "        session=session,\n",
    "        #processes=2  # Use 2 parallel download processes\n",
    "    )\n",
    "\n",
    "# List the downloaded files\n",
    "print(listdir('path/to/output/GRD'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe6a94",
   "metadata": {},
   "source": [
    "## Dwonloading Sen1 data using multiple shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Iterating over each shapefile and saving in the corresponding folder\n",
    "\n",
    "from os import listdir, makedirs\n",
    "import os\n",
    "import getpass\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import asf_search as asf\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# User authentication\n",
    "username = input('Username:')\n",
    "password = getpass.getpass('Password:')\n",
    "\n",
    "# Start ASF session with credentials\n",
    "session = asf.ASFSession().auth_with_creds(username=username, password=password)\n",
    "\n",
    "# Define the folder where shapefiles are located\n",
    "shapefile_folder = 'path/'  # Update this with the folder containing your shapefiles\n",
    "\n",
    "# Define date range\n",
    "start_date = '2022-10-01'  # Update with your start date\n",
    "end_date = '2022-11-30'    # Update with your end date\n",
    "\n",
    "# Function to download a single product and save in specific folder\n",
    "def download_product(product, download_path):\n",
    "    product.download(path=download_path, session=session)\n",
    "\n",
    "# Function to process each shapefile and download Sentinel-1 data\n",
    "def process_shapefile(shapefile_path):\n",
    "    # Load the shapefile as AOI\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    aoi_geom = gdf.geometry.union_all().wkt  # Updated for handling the geometry\n",
    "\n",
    "    # Search for Sentinel-1 GRD HD data with the specified date range\n",
    "    results = asf.geo_search(\n",
    "        intersectsWith=aoi_geom,\n",
    "        platform=asf.PLATFORM.SENTINEL1,\n",
    "        processingLevel=asf.PRODUCT_TYPE.GRD_HD,\n",
    "        start=datetime.strptime(start_date, '%Y-%m-%d'),\n",
    "        end=datetime.strptime(end_date, '%Y-%m-%d'),\n",
    "        maxResults=20\n",
    "    )\n",
    "\n",
    "    # Create a folder named after the shapefile to save the images\n",
    "    shapefile_name = os.path.splitext(os.path.basename(shapefile_path))[0]\n",
    "    download_folder = f'path/to/data/download/GRD/{shapefile_name}'  # Folder named after shapefile\n",
    "    if not os.path.exists(download_folder):\n",
    "        makedirs(download_folder)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Download with progress bar using tqdm and parallel processes\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Use 2 parallel threads\n",
    "        list(tqdm(executor.map(lambda product: download_product(product, download_folder), results), \n",
    "                  total=len(results), \n",
    "                  desc=f\"Downloading GRD HD Images for {shapefile_name}\", \n",
    "                  unit=\"file\"))\n",
    "\n",
    "# Iterate through all shapefiles in the folder and process them\n",
    "for shapefile in listdir(shapefile_folder):\n",
    "    if shapefile.endswith('.shp'):\n",
    "        shapefile_path = os.path.join(shapefile_folder, shapefile)\n",
    "        print(f\"Processing shapefile: {shapefile}\")\n",
    "        process_shapefile(shapefile_path)\n",
    "\n",
    "# List the downloaded files\n",
    "for shapefile in listdir(shapefile_folder):\n",
    "    if shapefile.endswith('.shp'):\n",
    "        shapefile_name = os.path.splitext(shapefile)[0]\n",
    "        download_folder = f'path/to/data/download/GRD/{shapefile_name}'\n",
    "        print(f\"Files in {download_folder}: {listdir(download_folder)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc75ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
